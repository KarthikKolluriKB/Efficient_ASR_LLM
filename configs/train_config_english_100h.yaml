# =============================================================================
# SLAM-ASR Training Configuration - ENGLISH 100 HOURS
# LibriSpeech / Common Voice English - Full Training
# =============================================================================
# PURPOSE: Train on 100+ hours of English data to achieve competitive WER
# 
# KEY FIXES from previous run:
#   1. LayerNorm in projector - matches audio/text embedding scales
#   2. Lower projector downsampling (3x instead of 5x) - preserves detail
#   3. Warmup + lower LR - more stable convergence
#   4. Larger batch via gradient accumulation
#
# EXPECTED RESULTS: 15-25% WER (vs 34% before fixes)
# TARGET (with 960h): 5-10% WER
# =============================================================================

model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  
  # LLM - Qwen2.5-3B
  llm_model_name: qwen2.5-3B
  llm_model: Qwen/Qwen2.5-3B
  llm_dim: 2048
  llm_model_path: null
  llm_type: decoder_only
  
  # Whisper encoder - small for better representations
  encoder_model_name: whisper-small
  encoder_model: small
  encoder_ds_rate: 2
  encoder_model_path: null
  encoder_dim: 768            # small uses 768 (base was 512)            # small uses 768 (base was 512)
  encoder_num_layers: null  # Use all layers
  
  # Projector - REDUCED DOWNSAMPLING for better detail
  projector: linear
  projector_ds_rate: 3       # Changed from 5 -> 3 (preserves more info)
  projector_hidden_dim: 2048
  projector_dropout: 0.1
  modal: audio
  normalize: false
  encoder_type: finetune

train:
  model_name: ASRLLM
  run_validation: true
  
  # Batch settings - effective batch = 64 for stable gradients
  batch_size: 16
  val_batch_size: 8
  gradient_accumulation_steps: 4   # Increased: 16*4 = 64 effective
  num_workers: 4
  max_eval_batches: 100
  max_new_tokens: 128
  
  # Generation parameters
  repetition_penalty: 1.3
  
  # Training settings
  num_epochs: 15             # More epochs for larger data
  total_steps: 200000
  warmup_steps: 1000         # Longer warmup for stability
  validation_interval: 500
  
  # Optimizer - LOWER LR for stable training with LayerNorm
  lr: 5.0e-5                 # Reduced from 1e-4 (LayerNorm is sensitive)
  weight_decay: 0.01         # Reduced from 0.05
  grad_clip: 1.0
  gamma: 0.85
  
  # Memory optimization
  use_fp16: false
  mixed_precision: true
  quantization: false
  
  # Freeze encoder and LLM, only train projector
  freeze_llm: true
  freeze_encoder: true
  freeze_layers: false
  num_freeze_layers: 0
  
  # Output
  output_dir: outputs/slam_asr_english_100h/
  save_model: true
  seed: 42
  one_gpu: true
  use_peft: false
  use_fast_kernels: false
  batching_strategy: packing
  context_length: 512        # Increased for longer sequences

scheduler: 
  name: cosine_with_warmup
  total_training_steps: 200000
  warmup_steps: 1000

early_stopping: 
  monitor: val/wer
  mode: min 
  patience: 5
  min_delta: 0.005           # More sensitive stopping

data:
  dataset: speech_dataset
  file: datamodule/dataset.py:get_speech_dataset
  
  # Paths - UPDATE THESE FOR YOUR REMOTE PC
  train_data_path: data/common_voice_en/train.jsonl
  val_data_path: data/common_voice_en/validation.jsonl
  test_data_path: data/common_voice_en/test.jsonl

  # Audio settings
  input_type: mel
  mel_size: 80
  normalize: false
  
  # Variable length audio
  use_variable_length: true
  max_audio_length: 30       # Max 30 seconds
  fix_length_audio: -1
  
  # Text settings
  max_target_chars: 500
  
  inference_mode: false

# Logging
log:
  use_wandb: true
  wandb_dir: ./wandb
  wandb_entity_name: null
  wandb_project_name: SLAM_ASR_English_100h
  wandb_exp_name: english_100h_layernorm_ds3
  log_dir: ./logs
  log_filename: train_english_100h.log
  log_interval: 50
