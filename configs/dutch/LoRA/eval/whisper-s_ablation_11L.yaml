# =============================================================================
# Evaluation Config - Whisper-small Baseline (11 Layers) + LoRA
# =============================================================================
# Usage:
#   python eval.py --config configs/dutch/LoRA/eval/whisper-s_ablation_11L.yaml
#   python eval.py --config configs/dutch/LoRA/eval/whisper-s_ablation_11L.yaml --num_beams 2
#   python eval.py --config configs/dutch/LoRA/eval/whisper-s_ablation_11L.yaml --ckpt_path path/to/checkpoint.pt
# =============================================================================

# -----------------------------------------------------------------------------
# Model Configuration (must match training config!)
# -----------------------------------------------------------------------------
model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  
  # LLM settings
  llm_model_name: qwen2.5-3B
  llm_model: Qwen/Qwen2.5-3B
  llm_dim: 2048
  llm_model_path: null
  llm_type: decoder_only
  
  # Encoder settings
  encoder_model_name: whisper-small
  encoder_model: small
  encoder_ds_rate: 2
  encoder_model_path: null
  encoder_dim: 768
  encoder_num_layers: 11       
  
  # Projector settings
  projector: concatLinear
  projector_ds_rate: 5
  projector_hidden_dim: 2048
  projector_dropout: 0.1
  modal: audio
  normalize: false
  encoder_type: finetune

# -----------------------------------------------------------------------------
# Evaluation Configuration
# -----------------------------------------------------------------------------
eval:
  # Checkpoint path (can be overridden via CLI --ckpt_path)
  # NOTE: LoRA checkpoints are named checkpoint_best_wer.pt (not projector_best_wer.pt)
  projector_path: outputs/dutch/whisper-s_ablation_11L/checkpoint_best_wer.pt
  
  # Batch size (smaller than training due to beam search memory)
  batch_size: 8
  num_workers: 4
  
  # Generation settings
  max_new_tokens: 128            # Max tokens to generate
  num_beams: 2                   # Beam search size (1=greedy, 4=beam search)
  repetition_penalty: 1.0        # Penalty for repeated tokens (1.0=no penalty)
  length_penalty: 1.0            # Length penalty (1.0=neutral, >1=longer, <1=shorter)
  temperature: 1.0               # Sampling temperature (only used if do_sample=True)
  do_sample: false               # Use sampling (False=deterministic beam search)
  
  # Output settings
  output_dir: outputs/dutch/whisper-s_ablation_11L/eval/
  save_predictions: true

# -----------------------------------------------------------------------------
# Train Configuration (needed for model building - must match training!)
# -----------------------------------------------------------------------------
train:
  model_name: ASRLLM
  batch_size: 8
  val_batch_size: 8
  num_workers: 4
  output_dir: outputs/dutch/whisper-s_ablation_11L/
  freeze_llm: false              # Must be false for LoRA
  freeze_encoder: true
  mixed_precision: true
  quantization: false
  one_gpu: true
  use_peft: true
  
  # LoRA Configuration (must match training config!)
  use_lora: true
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules:
      - q_proj
      - v_proj
      - k_proj
      - o_proj

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  dataset: speech_dataset
  file: datamodule/dataset.py:get_speech_dataset
  hf_dataset_path: data/cv22_hf/nl
  train_split: train
  val_split: validation
  test_split: test
  use_raw_transcription: false
  input_type: mel
  mel_size: 80
  normalize: false
  projector_ds_rate: 5
  use_variable_length: true
  max_audio_length: 30
  fix_length_audio: -1
  max_target_chars: 500
  inference_mode: true           

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
log:
  use_wandb: true
  wandb_dir: ./wandb
  wandb_entity_name: null
  wandb_project_name: SLAM_ASR_Dutch_ABLATIONS_LoRA
  wandb_exp_name: eval_whisper-s_ablation_11L
  log_dir: ./logs
  log_filename: eval_ablation_11L_nl.log