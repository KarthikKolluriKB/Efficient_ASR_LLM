# =============================================================================
# SLAM-ASR Training Configuration
# Common Voice Danish - SPEAKABLE 2026
# =============================================================================
# Usage: python train.py --config configs/train_config.yaml
# =============================================================================

model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  
  # LLM - Qwen2.5-3B (multilingual, supports Danish)
  llm_model_name: qwen2.5-3B
  llm_model: Qwen/Qwen2.5-3B
  llm_dim: 2048
  llm_model_path: null
  llm_type: decoder_only
  
  # Whisper encoder - base multilingual (supports Danish)
  encoder_model_name: whisper-base
  encoder_model: base  # NOT "base.en" - need multilingual!
  encoder_ds_rate: 2
  encoder_model_path: null
  encoder_dim: 512
  encoder_num_layers: null  # Number of encoder layers to use (null = all 6, or 1-6)
  
  # Projector: maps encoder dim (512) -> LLM dim (2048)
  projector: linear
  projector_ds_rate: 5
  # Smaller hidden dim to prevent overfitting with small dataset (3.5K samples)
  # With 512 hidden: ~2.6M params vs 9.4M with 2048 hidden
  projector_hidden_dim: 512
  modal: audio
  normalize: false
  encoder_type: finetune

train:
  model_name: ASRLLM
  run_validation: true
  
  # Batch settings - smaller batch for small dataset (3.5K samples)
  # Smaller batch = more updates + gradient noise (natural regularization)
  batch_size: 16  # Effective batch = 16 * 2 = 32, steps/epoch = 112
  val_batch_size: 32  # Large for fast validation (no gradients = less memory)
  gradient_accumulation_steps: 2  # Total effective batch = 32
  num_workers: 8  # Increased for faster data loading
  max_eval_batches: 100  # Limit eval batches (2510/64 = 40 batches, well under limit)
  max_new_tokens: 128  # Max tokens to generate during eval
  
  # Generation parameters for evaluation
  repetition_penalty: 1.2  # Prevent repetitive outputs like "Human Human Human"
  
  # Training settings - MANY more epochs for projector-only training
  # With frozen LLM, projector needs 50-100 epochs to learn audio-text mapping
  num_epochs: 50  # More epochs for small dataset
  total_steps: 100000
  warmup_steps: 200  # ~2 epochs warmup (112 steps/epoch)
  validation_interval: 112  # Validate every epoch (3592 / 32 = 112)
  
  # Optimizer - lower LR for more stable projector training
  # Start with 1e-4 and let scheduler decay it
  lr: 1.0e-4
  weight_decay: 0.01
  grad_clip: 1.0
  gamma: 0.85
  
  # Memory optimization
  use_fp16: false
  mixed_precision: true
  quantization: false
  
  # SLAM-ASR: Freeze encoder and LLM, only train projector
  freeze_llm: true
  freeze_encoder: true
  freeze_layers: false
  num_freeze_layers: 0
  
  # Output
  output_dir: outputs/whisp_b_concatlin_med_qwen_danish/
  save_model: true
  seed: 42
  one_gpu: true
  use_peft: false
  use_fast_kernels: false
  batching_strategy: packing
  context_length: 384

scheduler: 
  name: cosine_with_warmup
  total_training_steps: 5000
  warmup_steps: 200

early_stopping: 
  monitor: val/wer  # Monitor WER instead of loss (lower is better)
  mode: min 
  patience: 15  # 15 epochs without improvement
  min_delta: 0.005  # 0.5% WER improvement required

data:
  dataset: speech_dataset
  file: datamodule/dataset.py:get_speech_dataset
  
  # Common Voice Danish paths
  train_data_path: data/common_voice_da/train.jsonl
  val_data_path: data/common_voice_da/validation.jsonl
  test_data_path: data/common_voice_da/test.jsonl
  
  train_split: train
  val_split: val
  test_split: test
  
  # Audio settings
  input_type: mel
  mel_size: 80
  normalize: false
  use_variable_length: true
  max_audio_length: 20  # Reduced from 30s to prevent OOM spikes
  fix_length_audio: -1
  inference_mode: false
  
  prompt: null
  data_path: null
  max_words: null
  max_mel: null

log:
  use_wandb: true
  wandb_dir: ./wandb
  wandb_entity_name: null  # Uses your default wandb account
  wandb_project_name: SLAM_ASR_Danish
  wandb_exp_name: whisp_b_concatlin_med_qwen_danish
  log_dir: ./logs
  log_filename: train.log
  log_interval: 50
