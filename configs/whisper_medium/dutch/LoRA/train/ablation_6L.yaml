# =============================================================================
# Experiment: Whisper-medium Ablation (6 Layers) + LoRA
# =============================================================================
# Training config for Dutch Common Voice
# Whisper-medium (6 encoder layers) + Qwen2.5-3B + concatLinear projector

# Calculation:
#   - samples: 40,000
#   - batch_size: 8
#   - steps_per_epoch: 40,000 / 8 = 5,000
#   - num_epochs: 6
#   - total_steps: 5,000 Ã— 6 = 30,000
#   - warmup_steps: 5% of total = 1,500
#   - validation_interval: 5% of total_steps = 5% of 30000 = 1500
# ==============================================================================

model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  
  llm_model_name: qwen2.5-3B
  llm_model: Qwen/Qwen2.5-3B
  llm_dim: 2048
  llm_model_path: null
  llm_type: decoder_only
  
  encoder_model_name: whisper-medium
  encoder_model: medium
  encoder_ds_rate: 2
  encoder_model_path: null
  encoder_dim: 1024              # medium uses 1024
  encoder_num_layers: 6         # 6 layers for ablation
  
  projector: concatLinear
  projector_ds_rate: 5
  projector_hidden_dim: 2048
  projector_dropout: 0.1
  modal: audio
  normalize: false
  encoder_type: finetune

train:
  model_name: ASRLLM
  run_validation: true
  
  # Batch Configuration
  batch_size: 8
  val_batch_size: 8
  gradient_accumulation_steps: 1
  num_workers: 4
  max_eval_batches: 200
  
  # Generation Configuration
  max_new_tokens: 128
  repetition_penalty: 1.5
  
  # Training Schedule
  num_epochs: 6
  total_steps: 30000
  warmup_steps: 1500
  validation_interval: 1500
  # Optimizer Configuration
  lr: 1.0e-4
  weight_decay: 0.01
  grad_clip: 1.0
  lora_lr_multiplier: 1.0         # LoRA LR = lr * multiplier
  
  # Precision Configuration
  use_fp16: false
  mixed_precision: true
  quantization: false
  
  # ==========================================================================
  # FREEZING CONFIGURATION - Projector + LoRA
  # ==========================================================================
  freeze_encoder: true      # Whisper encoder is frozen
  freeze_llm: true          # Ignored when use_lora=true (PEFT handles it)
  use_lora: true            # Enable LoRA on LLM
  
  # LoRA Configuration
  # IMPORTANT: This must be inside the 'train:' section!
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules:
      - q_proj
      - v_proj
      - k_proj
      - o_proj
  # ==========================================================================
  
  # Output Configuration
  output_dir: outputs/whisper_medium/dutch/LoRA/whisper-medium_ablation_6L_lora_final/
  save_model: true
  seed: 42
  
  # Other Configuration
  one_gpu: true
  use_peft: true
  use_fast_kernels: false
  batching_strategy: packing
  context_length: 384

scheduler: 
  name: cosine_with_warmup
  total_training_steps: 30000
  warmup_steps: 1500
  min_lr_ratio: 0.1

early_stopping: 
  monitor: val/wer            # Monitor validation WER
  mode: min                   # Lower is better
  patience: 2                # Wait 2 epochs without improvement before stopping
  min_delta: 0.003            # Minimum change to qualify as improvement (0.3% WER)


data:
  dataset: speech_dataset
  file: datamodule/dataset.py:get_speech_dataset
  hf_dataset_path: data/cv22_hf/nl
  train_split: train
  val_split: validation
  test_split: test
  use_raw_transcription: false
  input_type: mel
  mel_size: 80                    
  normalize: false
  projector_ds_rate: 5
  use_variable_length: true
  max_audio_length: 30            
  fix_length_audio: -1
  max_target_chars: 500
  inference_mode: false

log:
  use_wandb: true
  wandb_dir: ./wandb
  wandb_entity_name: null
  wandb_project_name: SLAM_ASR_whisper_medium_Dutch_ABLATIONS_LoRA
  wandb_exp_name: whisper-medium_ablation_6L_lora_final
  log_dir: ./logs
  log_filename: whisper-medium_ablation_6L_lora_final.log
  log_interval: 100